\documentclass[letterpaper,12pt]{article}
\usepackage[utf8x]{inputenc}

\usepackage{geometry}
\usepackage{setspace}
\doublespacing

%opening
\title{Refmanage}
\author{Joshua Ryan Smith}	

\begin{document}

\maketitle




\section{Motivation}
Scholarship, writing, and learning are all fundamental components of science. Scholarship in this context is the ability to put one's own work in the context of what's been reported in \emph{The Literature}. All of these components rely on a large quantity of written information, so large that managing the information is a science unto itself. Dealing with this large amount of written material requires unavoidable work on the part of the scientist, but much of the work is inane, able to be automated, and best left to systems and machines. The scientist's expert time and energy is inefficiently wasted on this inanity and is better spent on the creative part of science. I need to systematize the process of dealing with the written information I require and have acquired, mostly in the form of books and scientific articles. All of the other solutions I've seen do not comprehensively solve the problem I have. Thus I need to create my own system, leveraging the tools I have at my disposal.









\section{Specification and Requirements}
In this section I will describe all of the requirements this system will have to meet. I will also go into detail describing the systems with which this system must interact. I will give use cases of this system. I will describe the cases this system will not address.

\subsection{Related Systems}
My refmanage system will interact most with reading and writing. In order to best understand the refmanage requirements and specifications it is critical to understand the context in which refmanage must operate.

\subsubsection{Writing}
I have most felt the need for a robust reference management system while writing my own scientific manuscripts. Because \LaTeX has such a powerful system for abstracting cross-references and citations, it is the obvious tool for generating documents. The standard citation management for \LaTeX is bibtex, and at the heart of bibtex is the bibliographic database. I have found that one monolithic database of bibliographic items is best, since there is to cost to having unused bibliographic items, but it is far too tedious to try to make custom bibliographic databases for individual manuscripts. Consider: having a custom Bib\TeX database for each manuscript means manually adding bibliographic items to the database each time a citation needs to be added to the manuscript. On the other hand, a monolithic database means that all the bibliographic items are already present and adding a new citation to the manuscript is as simple as adding a \verb|\cite{}| command in the manuscript source. The \LaTeX parser simply ignores all the unused bibliographic items in the database. Therefore, the Bib\TeX database should include database entries for absolutely every piece of written material I have ever read. This requirement leads into the next related system, Reading.

Practically speaking, writing usually involves generating a stack of literature and matching items in the stack to a certain location in the document I am creating. This map is then used to automatically generate citations based on the calls to the item in the stack and that item's metadata using bibtex. In fact, this process applies not just to writing manuscripts, but posters and slide decks, too.


\subsubsection{Reading}
Taking a step back, there exists a massive set of written material, and a subset of that material known as \emph{The Scientific Literature}, or \emph{The Literature} for short. Of this subset, there is a very small subset of the literature that is relevant to my work, and a very small subset comprising all of the written information I have read. Organizing and searching the literature is a science unto itself but fortunately there are a number of tools and system that are available to me to make fruitful searching of \emph{The Literature} a manageable task. Some of these tools are libraries and librarians, the internet, Google, Web of Science, PACS, digital object identifiers (DOI), international standard book number (ISBN), personal recommendations and my professional/social network, structures for formatting bibliographic data (RIS, bibtex), posts on the internet, and even \emph{The Literature} itself, in the form of lists of citations within manuscripts as well as what I call "forward references'' - works that cite a particular manuscript.

My main goal in reading is to learn; any individual text is a source of information as is the web of citations that traverses \emph{The Literature}. I do not know what particular manuscripts contain the information I need beforehand, so I typically start with keyword searches in a search engine to generate a kernel of manuscripts on a particular subject. In addition, I sometimes inherit this kernel from others either by receiving one or many manuscripts on a topic. For what it's worth, I'm defining "manuscript'' as an individual unit of \emph{The Literature}  - usually a book or scientific article but sometimes a post on the internet. I will read through this kernel, marking citations within this material that seem appropriate to look at later. Also it sometimes seems appropriate to look for the forward references for a particular manuscript. Libraries and librarians, Google Scholar, Web of Science, and publishers' websites all facilitate the discovery and collection of manuscripts. At the end of this process (there really isn't an end, just temporary exhaustion of all available means - should write more about this phenomenon), I have an understanding of the landscape and a set of manuscripts that is a representation of this understanding. It is the management of this set of manuscripts that motivates the need for refmanage.

I find it far better to read a hardcopy of a scientific paper or book rather than a PDF. I rarely print out webpages. I think that the two main parameters determining if I require a hardcopy are complexity/density of material and length.








\subsection{Requirements}
The main goal of this reference management system is to facilitate writing and reading by automating and removing the tedium of managing the collection of manuscripts I have read. In broad terms, there will be three main components of this system.

\begin{itemize}
\item Physical incarnations of manuscripts, e.g. printouts, books, etc.
\item Electronic incarnations of manuscripts, e.g. PDFs, webpages, etc.
\item A database with bibliographic data for each individual manuscript.
\end{itemize}

Hardcopies are typically very easy to interact with and therefore their presence as one of the components should be obvious. Given any individual hardcopy, it should be straightforward to find the reference in the database, particularly without having to directly query the database in order to include a citation in a manuscript I am writing. Furthermore, it should be easy to locate any particular hardcopy without having to re-print. When searching for and collecting manuscripts for reading, it should be straightforward to batch print the set of manuscripts.

Electronic incarnations of manuscripts should be located in one place on a filesystem. Each manuscript should be a single file, and the filename should be unique. It would be very helpful if the files were searchable because then the collection of files would be a knowledgebase and hopefully tools like spotlight could be used. It would also be useful if each file contained structured bibliographic metadata identical to the metadata in the database.

The database should include an item for every manuscript in the system. It should be automatic and very easy to import new items into the database, and there needs to be a pointer to the physical location of the hardcopy as well as a link to the electronic incarnation in the filesystem. I need a way to collect new manuscripts similar to GTD, and I need a system to manage hardcopies during the reading process. the bibliographic data should be complete and it should be a rare occurrence for me to manually key in an entry to the database. The database needs to be able to export to bibtex. Importing a single item or list of items should be equally easy. Importing an item into the database should simultaneously generate a hardcopy and an electronic copy appropriately linked an labeled and in the appropriate location.

This system should not be overly complex, difficult to use, or difficult to set up. It should also handle exceptions (information that isn't a book or paper) gracefully. It should also be able to deal with duplicates, e.g. I download the same paper 5 times in a row. The system should have as much automation and as little by-hand-tweaking-and-data-entry as possible.

What kinds of things will the system manage? In short, anything I can cite in a paper. The two main entries are other scientific papers (uniquely identified by DOI), and books (uniquely identified by ISBN). Both can be stored as PDFs on my computer. Also, sections or pages of books.

I don't want to have to go back and import metadata on a case-by-case basis. It is better if I get it and don't need it than need it and then have to get it.

\begin{itemize}
\item BibTeXML and Pybtex are probably better for database storage than bibtex.
\item The filename of the PDF should also be its DOI (modified with double underscore replacing the slash character).
\item BibTeXML: both Pybtex and tellico can parse this kind of file.
\item I would like covers and first pages as images for all the entries in the database. Scanning, pdftk, and imagemagik convert can make this dream a reality.
\end{itemize}

The real magic would be to be able to grab the bibliographic metadata simply by using the DOI.

NOTE: The fact that the .bib metadata file that comes from google scholar is different from the one that comes from the publisher is different from the one that comes from web of science bothers me. In other words, the sets have nonzero differences.






\subsection{Upgrading and evolution}
I will also need a system to handle changes to the workflow or database schema. For example, I had an ad-hoc scheme of assigning bibtex keys to items, the I switched to whatever default the .bib files came with, now I'm using DOI. Each time I make one of these changes, I wind up breaking backwards compatibility. I've already had lots of ideas on how to improve this thing like inserting metadata into the PDFs themselves, overlaying DOI, tellico ID, etc. on the PDFs themselves, etc. This system needs to be able to deal with those updates without breaking. Some of these issues can be solved by my version control system.



\subsection{Non-goals}
This version will not support the following features:

\begin{itemize}
\item Maintenance on the existing bibliographic database and/or system.
\item Fetching PDFs and bibliographic data from the internet given a DOI.
\end{itemize}









\section{General Design}
This section will contain block diagrams, workflows, flow diagrams, etc. It will also contain a subsection on how the system can be gracefully upgraded.

Here is a new workflow for dealing with only a pile of bibtex files. 

Start with a set of bibtex files. Cat the list into a tellico database. Check each item for DOI. Replace bibtex-key with DOI. Replace URL with dx.doi.org/<DOI>. Use python to follow the dx.doi.org/<DOI> link to the webpage. Use beautifulsoup to find the PDF. Download the PDF. Change the name of the PDF file to the modified DOI. Import the new tc db into the main db and copy the new pdfs into ~/Documents/library. (beautifulsoup, urllib2 (urlretrieve)).

Here is a sketch of the solution.
I'll have basically two components locally and probably a third solely for collaboration on the net. The two local components are a folder full of PDFs (possibly word docs, text files, HTML, \LaTeX, etc) and a database of metadata. There will be some implicit organization in the filesystem, but the user will have to look at the database to do much useful stuff. Again, the organizing principle is unique identifier; usually DOI or ISBN. I should be able to strip away everything else and as long as I retain the list of DOIs and ISBNs, I should be able to regenerate the entire database and set of PDFs without losing any data locally. Note: I need to consider my own papers in this database.

I want to think about this system as a machine with a hopper: as long as I can get the information into a format that goes into the hopper, the tedium of collecting the metadata and PDF should be dealt with by the computer. There will be exceptional cases where I have to import a lot of metadata by hand, the system should make this process as easy as possible.

Here is a stab at the workflow: I first get some references in some kind of inbox. These refs can be a (single or) list of pdfs somebody sends me, a printout somebody hands me, a list of refs from papers I've read, etc. That list is first turned into a corresponding list of DOIs or ISBNs. That list of DOIs or ISBNs is parsed, and the metadata for each item is retrieved. At the same time, the corresponding PDF is downloaded. In the metadata, the bibtex key is changed to the (modified) DOI or ISBN, the filename of the PDF is changed to the (modified) DOI or ISBN, and the metadata is embedded into the PDF. The PDF is put into the correct place in the filesystem and the metadata is imported into tellico.

When I write a paper, I simply export the tellico db as a bibtex file and reference the appropriate keys.

\subsection{What's left}
Here are some lists of possible errors or user input during runtime.

\begin{itemize}
\item Cleanup auxiliary files?
\item Location of library?
\item Import new .tc database into mater?
\item Location of master.tc?
\item Move mod-doi pdfs to library?
\item Updated master.bib?
\end{itemize}

There are also errors that can occur due to assumptions I'm making.

\begin{itemize}
\item What if there are duplicates in list of bibliographic references I am trying to import?
\item What if some of the items in the list of bibliographic references I am trying to import don't have DOI?
\end{itemize}

Additional thoughts.

\begin{itemize}
\item bibxml
\item clean up current master.tc
\item add metadata to pdf files.
\item OCR pdfs as necessary
\item do I need to keep copies of the pdf as downloaded from the publisher?
\item can I integrate this workflow/program even more with the internet (connotea/bibsonomy) or tellico? In other words, have a browser button that grabs bib data and pdf.
\item Bigger picture: knowledgebase, workflow for writing.
\item Grandfather scheme: making changes to the database scheme (bibtex\_key -> ISBN or DOI) shouldn't interfere with previous manuscripts I've written. I.e. I should be able to build old documents without search and replace \\cite commands.
\end{itemize}

Here are some thoughts on the bigger picture. Basically I want three big buckets: one containing electronic copies of all the papers I've downloaded. One containing all of the bibliographic metadata for all of those papers, and one containing an organized system of physical printouts of the papers. In this way I will have a straightforward point of entry to pull in citation metadata into my own manuscripts, and I will have a knowledgebase. Ostensibly I will be able to search through all of the pdfs for information.






\section{Future Improvements}
Like embedding bib metadata in PDFs or OCRing PDFs.





















\end{document}
